{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvV6DnE59Da8ew/fmJDQ9g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bijular/datascience/blob/master/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q0yWzzRbQpE"
      },
      "source": [
        "#Word2vec word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICFNOSqqX5D7"
      },
      "source": [
        "from gensim import models\r\n",
        "# import the pre-trained model of Google News 6B words\r\n",
        "model = models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\r\n",
        "\r\n",
        "# predifined functions from the gensim lib\r\n",
        "print(model.most_similar(positive=['woman', 'Doctor'], negative=['man']))\r\n",
        "\r\n",
        "print(model.doesnt_match(\"spring autumn winter tennis\".split()))\r\n",
        "\r\n",
        "print(model.similarity('woman', 'man'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jbOJKqNbXxv"
      },
      "source": [
        "#GloVe word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO4WNdR3cIYp",
        "outputId": "21cd34a1-6f9b-42ad-a91b-b2aa10c390fc"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-15 16:35:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-15 16:35:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-15 16:35:20--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.02MB/s    in 6m 51s  \n",
            "\n",
            "2021-02-15 16:42:12 (2.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnLVave_i92W",
        "outputId": "cced30cd-2bd6-47b3-96d3-836d551c7fa1"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
        "glove_input_file = 'glove.6B.100d.txt'\r\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\r\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgsIdUT7j99e",
        "outputId": "6ad8f546-7ed9-4297-f20b-8fe9616452c7"
      },
      "source": [
        "from gensim.models import KeyedVectors\r\n",
        "# load the Stanford GloVe model\r\n",
        "filename = 'glove.6B.100d.txt.word2vec'\r\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\r\n",
        "\r\n",
        "print(model.most_similar(positive=['woman', 'doctor'], negative=['man']))\r\n",
        "\r\n",
        "print(model.doesnt_match(\"spring autumn winter tennis\".split()))\r\n",
        "\r\n",
        "print(model.similarity('woman', 'man'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('nurse', 0.7735227346420288), ('physician', 0.7189429998397827), ('doctors', 0.6824328303337097), ('patient', 0.6750682592391968), ('dentist', 0.6726033687591553), ('pregnant', 0.6642460227012634), ('medical', 0.6520450115203857), ('nursing', 0.645348072052002), ('mother', 0.6393327116966248), ('hospital', 0.6387495994567871)]\n",
            "tennis\n",
            "0.8323495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7PWxkFbgn8"
      },
      "source": [
        "#FastText word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_QnsIzJkrjY"
      },
      "source": [
        "from gensim.models import FastText\r\n",
        "from gensim.test.utils import common_texts\r\n",
        "model = FastText( window=3, min_count=1)\r\n",
        "model.build_vocab(sentences=common_texts)\r\n",
        "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBZ5cNv2kdDN",
        "outputId": "b1273b0c-f288-486e-892d-93ac4a3523e5"
      },
      "source": [
        "similarities = model.wv.most_similar(positive=['computer', 'human'], negative=['interface'])\r\n",
        "print(similarities[0])\r\n",
        "\r\n",
        "print(model.wv.doesnt_match(\"human computer interface tree\".split()))\r\n",
        "\r\n",
        "print(model.wv.similarity('computer', 'human'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('response', 0.09965892136096954)\n",
            "tree\n",
            "-0.046674214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exkMZP3mbn0r"
      },
      "source": [
        "#LaBSE Sentence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HLud7oVODc2"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30i5xIwrQ-vd"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "def get_model(model_url, max_seq_length):\r\n",
        "  labse_layer = hub.KerasLayer(model_url, trainable=True)\r\n",
        "  # Define input.\r\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_word_ids\")\r\n",
        "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_mask\")\r\n",
        "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"segment_ids\") \r\n",
        "\r\n",
        "  # LaBSE layer.\r\n",
        "  pooled_output,_ = labse_layer([input_word_ids, input_mask, segment_ids])\r\n",
        "\r\n",
        "  # The embedding is l2 normalized.\r\n",
        "  pooled_output = tf.keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\r\n",
        "\r\n",
        "  # Define model.\r\n",
        "  \r\n",
        "  return tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids],outputs=pooled_output), labse_layer\r\n",
        "\r\n",
        "max_seq_length = 64\r\n",
        "labse_model, labse_layer = get_model(model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)\r\n",
        "\r\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF3akW9LL_Pg",
        "outputId": "69af6afa-49fa-4574-a216-11ce21a52935"
      },
      "source": [
        "\r\n",
        "import bert\r\n",
        "max_seq_length=64\r\n",
        "labse_layer = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/1\", trainable=True)\r\n",
        "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\r\n",
        "\r\n",
        "def create_input(input_strings, tokenizer, max_seq_length):\r\n",
        "  input_ids_all, input_mask_all, segment_ids_all = [], [], []\r\n",
        "  for input_string in input_strings:\r\n",
        "    # Tokenize input.\r\n",
        "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\r\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\r\n",
        "    sequence_length = min(len(input_ids), max_seq_length)\r\n",
        "\r\n",
        "\r\n",
        "    # Padding or truncation.\r\n",
        "    if len(input_ids) >= max_seq_length:\r\n",
        "      input_ids = input_ids[:max_seq_length]\r\n",
        "    else:\r\n",
        "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\r\n",
        "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\r\n",
        "    input_ids_all.append(input_ids)\r\n",
        "    input_mask_all.append(input_mask)\r\n",
        "    segment_ids_all.append([0] * max_seq_length) \r\n",
        "  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\r\n",
        "\r\n",
        "def encode(input_text):\r\n",
        "  input_ids, input_mask, segment_ids = create_input(\r\n",
        "      input_text, tokenizer, max_seq_length)\r\n",
        "  return labse_model([input_ids, input_mask, segment_ids])\r\n",
        "\r\n",
        "english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\r\n",
        "italian_sentences = [\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"]\r\n",
        "japanese_sentences = [\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"]\r\n",
        "\r\n",
        "english_embeddings = encode(english_sentences)\r\n",
        "italian_embeddings = encode(italian_sentences)\r\n",
        "japanese_embeddings = encode(japanese_sentences)\r\n",
        "\r\n",
        "# English-Italian similarity\r\n",
        "print (np.matmul(english_embeddings, np.transpose(italian_embeddings)))\r\n",
        "\r\n",
        "# English-Japanese similarity\r\n",
        "print (np.matmul(english_embeddings, np.transpose(japanese_embeddings)))\r\n",
        "\r\n",
        "# Italian-Japanese similarity\r\n",
        "print (np.matmul(italian_embeddings, np.transpose(japanese_embeddings)))\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4943e6d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4943e6d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.63192725 0.30619976 0.44297647]\n",
            " [0.11652887 0.8596669  0.35940808]\n",
            " [0.14804009 0.3244818  0.9542653 ]]\n",
            "[[0.93567264 0.5403088  0.46792448]\n",
            " [0.3180447  0.7622262  0.36086115]\n",
            " [0.36750826 0.42791754 0.8171463 ]]\n",
            "[[0.534371   0.25018716 0.19974771]\n",
            " [0.30141014 0.7133327  0.4064769 ]\n",
            " [0.38503203 0.47768122 0.8674307 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KKL0fuzNtQ4",
        "outputId": "7e5c98d6-c3dd-4fb7-b50d-8602a3ee9104"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 35.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp36-none-any.whl size=103068 sha256=e1f4a58648f72d99f1848f925a67635c782b92a615581595b60d15c97d9136e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=03760dd8810459596cc569f5291fa6f3b7340cb28149148eee28bef3d164c681\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}